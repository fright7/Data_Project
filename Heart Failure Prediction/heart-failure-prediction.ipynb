{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Heart Failure Prediction\n\n## Prerequisite\n\nPandas, Matplotlib, Seaborn, Sklearn\n\n## What we do?\n\nFirst I will do some EDA and visualization data. After that I will train model and find best model\n\n","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport seaborn as sns\n\n# Import things I need","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv\")\ndisplay(data)\ndisplay(data.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ### Display dataset. We can know that all data type is num. So we don't need process that encode char to num. All we need is normalize, standandarlize dataset.\n \n### When we see data, data range are different. For example \"platelets\" columns show higher figure than other columns. If we don't standardlize columns, accuracy of model can be decreased.\n\n### First we start with EDA and preprocessing dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"null_value = data.isnull().sum()\nprint(null_value)\n\n# We check whether there are null_values and there are no null_values so we can pass this process","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_check = [\"age\",\"creatinine_phosphokinase\",\"ejection_fraction\",\"platelets\",\"serum_creatinine\",\"serum_sodium\",\"time\"]\n\nplt.figure(figsize=(30,15))\nn1 = 0\nfor i in range(1,8):\n    plt.subplot(2,4,i)\n    plt.hist(data[val_check[n1]])\n    plt.title(val_check[n1])\n    n1 +=1\n    \n# We should also check whether data follow normal distribuion. We can find that \"creatinine_phosphokinase\" and \"serum_creatinine\" columns skewed a lot.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def logarithm(x):\n    return np.log(x+1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_2 = data.copy()\n\nlog1 = data.creatinine_phosphokinase.apply(lambda x : logarithm(x)).to_frame()\nlog2 = data.serum_creatinine.apply(lambda x : logarithm(x)).to_frame()\n\ndf_2.drop([\"creatinine_phosphokinase\",\"serum_creatinine\"],axis=1,inplace=True)\n\n\ndisplay(df_2)\n\n# Normalize data through logarithm function.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_3 = pd.concat([df_2,log1,log2],axis=1)\ndisplay(df_3)\n\nresult_val = [\"creatinine_phosphokinase\",\"serum_creatinine\"]\n\nplt.figure(figsize=(10,10))\nfor j in range(0,2):\n    plt.subplot(1,2,j+1)\n    plt.hist(df_3[result_val[j]])\n    plt.title(result_val[j])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\n# As I mentioned earlier, we should stasndardilize each variables so that one variable can not affect to the result more than other variables. So I use MinMaxScaler ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = MinMaxScaler()\ndf_4 = pd.DataFrame(scaler.fit_transform(df_3),columns=df_3.columns)\ndisplay(df_4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = df_4.corr()\nplt.figure(figsize=(16,16))\ncmap = sns.cubehelix_palette(as_cmap=True)\nsns.heatmap(corr,fmt=\".2f\",annot=True,cmap=cmap,vmin=0.2)\n\n# When we see \"Death_event\", \"age\" and \"serum_creatinine\" affect most","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We finish seeing and preprocessing dataset. After this, we will train model and evaluate model through accuracy score and f1 score","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import fbeta_score\n\n# For classification I choose RandomForestClassifier, AdaBoostClassifier and Support Vector machine.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_4.loc[:,df_4.columns != \"DEATH_EVENT\"]\ny = df_4.loc[:,\"DEATH_EVENT\"]\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state=62)\n\n# Split data and define X_train, X_test, y_train, y_test to train model. I define y variable as \"Death event\" which indicate whether patient deceased.\n# I import Fbeta_score and I will weight more to recall rather than precision because data that I classified should reflect well about real data. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = RandomForestClassifier()\nmodel1.fit(X_train,y_train)\nmodel1_preds = model1.predict(X_test)\naccuracy1 = accuracy_score(y_test,model1_preds)\nfbeta_1 = fbeta_score(y_test,model1_preds,beta=1.5)\nprint(\"Accuracy of RandomForestClassifier : {}  fbeta score : {}\".format(accuracy1,fbeta_1))\n\n# First I train RandomForestClassifier and test it. Accuracy_score is 0.84 and f1_score is 0.74. Not bad","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = AdaBoostClassifier()\nmodel2.fit(X_train,y_train)\nmodel2_preds = model2.predict(X_test)\nAccuracy2 = accuracy_score(y_test,model2_preds)\nfbeta_2 = fbeta_score(y_test,model2_preds,beta=1.5)\nprint(\"Accuracy of AdaBoostClassifier : {} fbeta score : {}\".format(Accuracy2,fbeta_2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model3 = SVC()\nmodel3.fit(X_train,y_train)\nmodel3_preds = model3.predict(X_test)\nAccuracy3 = accuracy_score(y_test,model3_preds)\nfbeta_3 = fbeta_score(y_test,model3_preds,beta=1.5)\nprint(\"Accuracy of SVC : {} fbeta score : {}\".format(Accuracy3,fbeta_3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### As a result, RandomForestClassifier shows highest Accuracy score and fbeta score so I will choose model1. And through this model I will do gridsearch so that I can improve my model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\nimport time\n\n# Import things that I need. Also I import make_scorer for scoring. In this case I also use fbeta_score that has beta = 1.5. \n# Also I import time so that I can measure time for searching best estimator.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scorer = make_scorer(fbeta_score,beta=1.5)\nparameters = {\n    \"n_estimators\" : [100,150,200,250,300],\n    \"min_samples_split\" : [2,4,6],\n    \"min_samples_leaf\" : [4,6,8],\n    \"max_depth\" : [80,100,150,200]\n    \n}\n\nstart = time.time()\ngrid = GridSearchCV(estimator=model1,param_grid=parameters,scoring=scorer,n_jobs=-1,cv=2)\ngrid.fit(X_train,y_train)\nend = time.time()\n\nprint(\"Search Time : {} seconds\".format(end-start))\n\ngrid.best_params_\n\n# Check RandomForestClassifier parameters here \"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model4 = grid.best_estimator_\nmodel4_preds = model4.predict(X_test)\nAccuracy4 = accuracy_score(y_test,model4_preds)\nfbeta_4 = fbeta_score(y_test,model4_preds,beta=1.5)\nprint(\"GridSearch accuracy : {} fbeta score : {}\".format(Accuracy4,fbeta_4))\n\n# Through GridSearch I can improve my model. Accuracy : 0.84 → 0.86 Fbeta : 0.74 → 0.78","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Importance = np.sort(np.round(model4.feature_importances_*100,3))\ndf_feature = pd.DataFrame({\n    \"importance\" : Importance\n},index=X_train.columns)\ndisplay(df_feature)\n\nplt.figure(figsize=(16,16))\nplt.barh(df_feature.index.to_list(),df_feature.importance)\nplt.title(\"Feature importance\")\n\n# Through RandomForestClassifier, we can check feature importance. Through using numpy, sort values and make dataframe.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### This is end of my code. Thank you for seeing my code!","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}